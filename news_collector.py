#!/usr/bin/env python3
"""
ü§ñ SISTEMA DE COLETA AUTOM√ÅTICA DE NOT√çCIAS TEM VENDA
Coleta not√≠cias de sites farmac√™uticos usando IA para an√°lise e categoriza√ß√£o
"""

import os
import json
import requests
import feedparser
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import hashlib
import re
from supabase import create_client, Client
from openai import OpenAI
import time
import logging

# Carregar vari√°veis de ambiente do arquivo .env se existir
def load_env_file():
    """Carrega vari√°veis de ambiente de um arquivo .env"""
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

load_env_file()

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_collector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        # Configura√ß√µes Supabase
        self.supabase_url = "https://mgcoyeohqelystqmytah.supabase.co"
        self.supabase_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im1nY295ZW9ocWVseXN0cW15dGFoIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MTY3MDM2NCwiZXhwIjoyMDc3MjQ2MzY0fQ.wylX0wMD5teTcADuUvU81R1bft3pftGhhU-BGKYv9TQ"
        
        # Configura√ß√µes OpenAI
        self.openai_key = os.getenv('OPENAI_API_KEY')
        
        # Inicializar clientes
        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)
        self.openai = OpenAI(api_key=self.openai_key) if self.openai_key else None
        
        # Configura√ß√µes
        self.max_articles_per_run = 20
        self.min_content_length = 200
        
        # Palavras-chave para filtrar not√≠cias relevantes
        self.keywords = [
            'farm√°cia', 'farmac√™utico', 'medicamento', 'drogaria',
            'anvisa', 'regulamenta√ß√£o', 'gen√©rico', 'similar',
            'vendas farmac√™uticas', 'mercado farmac√™utico',
            'gest√£o farm√°cia', 'lideran√ßa farm√°cia'
        ]
        
        logger.info("üöÄ NewsCollector inicializado com sucesso!")

    def get_active_sources(self):
        """Busca fontes ativas no banco"""
        try:
            response = self.supabase.table('news_sources').select('*').eq('is_active', True).execute()
            return response.data
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar fontes: {e}")
            return []

    def fetch_rss_articles(self, source):
        """Coleta artigos de uma fonte RSS"""
        articles = []
        
        if not source.get('rss_url'):
            logger.warning(f"‚ö†Ô∏è Fonte {source['name']} n√£o tem RSS URL")
            return articles
            
        try:
            logger.info(f"üì° Coletando RSS de: {source['name']}")
            feed = feedparser.parse(source['rss_url'])
            
            for entry in feed.entries[:10]:  # Limitar a 10 por fonte
                try:
                    article = {
                        'title': entry.get('title', ''),
                        'url': entry.get('link', ''),
                        'excerpt': entry.get('summary', ''),
                        'published_at': self.parse_date(entry.get('published')),
                        'source_id': source['id'],
                        'raw_content': entry.get('summary', '')
                    }
                    
                    # Verificar se √© relevante
                    if self.is_relevant_article(article):
                        articles.append(article)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar entrada RSS: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao coletar RSS de {source['name']}: {e}")
            
        return articles

    def scrape_article_content(self, url):
        """Extrai conte√∫do completo de um artigo"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remover scripts e estilos
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Tentar encontrar o conte√∫do principal
            content_selectors = [
                'article', '.article-content', '.post-content', 
                '.entry-content', '.content', 'main'
            ]
            
            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    break
            
            if not content:
                # Fallback: pegar todo o texto do body
                body = soup.find('body')
                if body:
                    content = body.get_text(strip=True)
            
            # Limitar tamanho
            if len(content) > 5000:
                content = content[:5000] + "..."
                
            return content
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair conte√∫do de {url}: {e}")
            return ""

    def is_relevant_article(self, article):
        """Verifica se o artigo √© relevante para farm√°cias"""
        text = f"{article['title']} {article['excerpt']}".lower()
        
        # Verificar palavras-chave
        relevance_score = sum(1 for keyword in self.keywords if keyword in text)
        
        # Verificar tamanho m√≠nimo
        if len(article['excerpt']) < self.min_content_length:
            return False
            
        return relevance_score >= 2  # Pelo menos 2 palavras-chave

    def analyze_with_ai(self, article):
        """Usa IA para analisar e categorizar o artigo"""
        if not self.openai:
            logger.warning("‚ö†Ô∏è OpenAI n√£o configurada, usando an√°lise b√°sica")
            return self.basic_analysis(article)
        
        try:
            prompt = f"""
            Voc√™ √© um especialista em gest√£o comercial farmac√™utica. Analise este artigo e forne√ßa insights estrat√©gicos para gestores de farm√°cias.
            
            T√≠tulo: {article['title']}
            Conte√∫do: {article['content'][:2000]}
            
            Responda em JSON com an√°lise completa:
            {{
                "category": "regulamentacao|mercado|tecnologia|gestao|saude-publica",
                "tags": ["tag1", "tag2", "tag3"],
                "priority": 0|1|2,
                "summary": "resumo em 2-3 frases",
                "relevance_score": 1-10,
                "commercial_analysis": {{
                    "business_impact": "Como isso impacta o neg√≥cio farmac√™utico (alta/m√©dia/baixa)",
                    "sales_opportunities": "Oportunidades de vendas identificadas",
                    "competitive_advantage": "Como usar isso para vantagem competitiva",
                    "action_items": "A√ß√µes pr√°ticas que o gestor pode tomar",
                    "risk_factors": "Riscos ou desafios para o neg√≥cio",
                    "market_trends": "Tend√™ncias de mercado identificadas"
                }},
                "executive_summary": "Resumo executivo para tomada de decis√£o em 1 par√°grafo"
            }}
            """
            
            response = self.openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=800,
                temperature=0.3
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na an√°lise IA: {e}")
            return self.basic_analysis(article)

    def basic_analysis(self, article):
        """An√°lise b√°sica sem IA"""
        text = f"{article['title']} {article['content']}".lower()
        
        # Categoriza√ß√£o b√°sica
        if any(word in text for word in ['anvisa', 'regulamenta√ß√£o', 'lei', 'norma']):
            category = 'regulamentacao'
        elif any(word in text for word in ['mercado', 'vendas', 'crescimento', 'receita']):
            category = 'mercado'
        elif any(word in text for word in ['tecnologia', 'digital', 'app', 'sistema']):
            category = 'tecnologia'
        elif any(word in text for word in ['gest√£o', 'lideran√ßa', 'equipe', 'treinamento']):
            category = 'gestao'
        else:
            category = 'saude-publica'
        
        # Tags b√°sicas
        tags = []
        if 'farm√°cia' in text:
            tags.append('farmacia')
        if 'medicamento' in text:
            tags.append('medicamentos')
        if 'anvisa' in text:
            tags.append('anvisa')
        
        # An√°lise comercial b√°sica
        business_impact = 'm√©dia'
        if any(word in text for word in ['crescimento', 'aumento', 'expans√£o', 'novo']):
            business_impact = 'alta'
        elif any(word in text for word in ['redu√ß√£o', 'diminui√ß√£o', 'crise', 'problema']):
            business_impact = 'baixa'
        
        sales_opportunities = "Analise oportunidades de vendas baseadas no conte√∫do da not√≠cia"
        competitive_advantage = "Identifique como usar esta informa√ß√£o para se destacar da concorr√™ncia"
        action_items = "Defina a√ß√µes pr√°ticas baseadas nas informa√ß√µes apresentadas"
        risk_factors = "Avalie riscos e desafios mencionados na not√≠cia"
        market_trends = "Identifique tend√™ncias de mercado relevantes"
        
        return {
            'category': category,
            'tags': tags[:3],
            'priority': 1 if 'urgente' in text or 'importante' in text else 0,
            'summary': article['excerpt'][:200] + "...",
            'relevance_score': 7,
            'commercial_analysis': {
                'business_impact': business_impact,
                'sales_opportunities': sales_opportunities,
                'competitive_advantage': competitive_advantage,
                'action_items': action_items,
                'risk_factors': risk_factors,
                'market_trends': market_trends
            },
            'executive_summary': f"Not√≠cia sobre {category} com impacto {business_impact} no neg√≥cio farmac√™utico. Recomenda-se an√°lise detalhada para identificar oportunidades de crescimento."
        }

    def generate_slug(self, title):
        """Gera slug √∫nico para o artigo"""
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug)
        slug = slug[:100]  # Limitar tamanho
        
        # Adicionar timestamp para garantir unicidade
        timestamp = int(time.time())
        return f"{slug}-{timestamp}"

    def save_article(self, article, analysis):
        """Salva artigo no banco de dados"""
        try:
            # Verificar se j√° existe
            existing = self.supabase.table('news_articles').select('id').eq('url', article['url']).execute()
            if existing.data:
                logger.info(f"üìÑ Artigo j√° existe: {article['title']}")
                return False
            
            # Buscar categoria
            category = self.supabase.table('news_categories').select('id').eq('slug', analysis['category']).execute()
            category_id = category.data[0]['id'] if category.data else None
            
            # Preparar dados
            article_data = {
                'title': article['title'],
                'slug': self.generate_slug(article['title']),
                'excerpt': analysis['summary'],
                'content': article['content'],
                'url': article['url'],
                'source_id': article['source_id'],
                'category_id': category_id,
                'status': 'pending',
                'priority': analysis['priority'],
                'published_at': article['published_at'],
                'scraped_at': datetime.now().isoformat()
            }
            
            # Adicionar an√°lise comercial se dispon√≠vel
            if 'commercial_analysis' in analysis:
                article_data['commercial_analysis'] = json.dumps(analysis['commercial_analysis'])
            
            if 'executive_summary' in analysis:
                article_data['executive_summary'] = analysis['executive_summary']
            
            # Inserir artigo
            result = self.supabase.table('news_articles').insert(article_data).execute()
            
            if result.data:
                article_id = result.data[0]['id']
                logger.info(f"‚úÖ Artigo salvo: {article['title']} (ID: {article_id})")
                
                # Salvar tags
                self.save_article_tags(article_id, analysis['tags'])
                
                return True
            else:
                logger.error(f"‚ùå Erro ao salvar artigo: {article['title']}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar artigo: {e}")
            return False

    def save_article_tags(self, article_id, tags):
        """Salva tags do artigo"""
        try:
            for tag_name in tags:
                # Buscar ou criar tag
                tag_result = self.supabase.table('news_tags').select('id').eq('name', tag_name).execute()
                
                if tag_result.data:
                    tag_id = tag_result.data[0]['id']
                else:
                    # Criar nova tag
                    new_tag = self.supabase.table('news_tags').insert({
                        'name': tag_name,
                        'slug': tag_name.lower().replace(' ', '-')
                    }).execute()
                    tag_id = new_tag.data[0]['id']
                
                # Relacionar artigo com tag
                self.supabase.table('news_article_tags').insert({
                    'article_id': article_id,
                    'tag_id': tag_id
                }).execute()
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao salvar tags: {e}")

    def parse_date(self, date_str):
        """Converte string de data para formato ISO"""
        if not date_str:
            return datetime.now().isoformat()
        
        try:
            # Tentar diferentes formatos
            formats = [
                '%a, %d %b %Y %H:%M:%S %z',
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt).isoformat()
                except ValueError:
                    continue
            
            return datetime.now().isoformat()
            
        except Exception:
            return datetime.now().isoformat()

    def run_collection(self):
        """Executa a coleta completa"""
        logger.info("üöÄ Iniciando coleta de not√≠cias...")
        
        sources = self.get_active_sources()
        if not sources:
            logger.warning("‚ö†Ô∏è Nenhuma fonte ativa encontrada")
            return
        
        total_collected = 0
        
        for source in sources:
            try:
                logger.info(f"üì° Processando fonte: {source['name']}")
                
                # Coletar artigos RSS
                articles = self.fetch_rss_articles(source)
                
                for article in articles[:5]:  # Limitar por fonte
                    try:
                        # Extrair conte√∫do completo
                        article['content'] = self.scrape_article_content(article['url'])
                        
                        if len(article['content']) < self.min_content_length:
                            continue
                        
                        # Analisar com IA
                        analysis = self.analyze_with_ai(article)
                        
                        # Salvar no banco
                        if self.save_article(article, analysis):
                            total_collected += 1
                        
                        # Limite por execu√ß√£o
                        if total_collected >= self.max_articles_per_run:
                            break
                            
                        # Pausa entre artigos
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao processar artigo: {e}")
                        continue
                
                # Atualizar √∫ltima coleta da fonte
                self.supabase.table('news_sources').update({
                    'last_scraped': datetime.now().isoformat()
                }).eq('id', source['id']).execute()
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar fonte {source['name']}: {e}")
                continue
        
        logger.info(f"‚úÖ Coleta conclu√≠da! {total_collected} artigos coletados")

def main():
    """Fun√ß√£o principal"""
    collector = NewsCollector()
    collector.run_collection()

if __name__ == "__main__":
    main()
